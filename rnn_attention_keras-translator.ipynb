{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import CuDNNLSTM as LSTM\n",
    "from keras.layers import Dense, Bidirectional, Concatenate, Input\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.layers import Lambda, Dot\n",
    "from keras.layers import RepeatVector \n",
    "from keras.layers import Embedding\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCES = 10000\n",
    "MAX_VOCABULARY = 20000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "EMBEDDINGS_DIM = 100\n",
    "LATENT_ENCODER_DIM = 200\n",
    "LATENT_DECODER_DIM = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_input_sentences = []\n",
    "spanish_output_sentences = []\n",
    "english_sentences = []\n",
    "\n",
    "count = 0\n",
    "with open(\"./spa.txt\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if(count< MAX_SENTENCES):\n",
    "            english, spanish = line.strip().split(\"\\t\");\n",
    "            spanish_input = \"<sos> \" + spanish; \n",
    "            spanish_output = spanish + \" <eos>\"\n",
    "            spanish_input_sentences.append(spanish_input);\n",
    "            spanish_output_sentences.append(spanish_output);\n",
    "            english_sentences.append(english);\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_tokenizer = Tokenizer(num_words = MAX_VOCABULARY)\n",
    "decoder_tokenizer = Tokenizer(num_words = MAX_VOCABULARY, filters = '.')\n",
    "#decoder_tokenizer = Tokenizer(num_words = MAX_VOCABULARY)\n",
    "encoder_tokenizer.fit_on_texts(english_sentences)\n",
    "decoder_tokenizer.fit_on_texts(spanish_input_sentences + spanish_output_sentences) #Dont remove special characers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_sequences = encoder_tokenizer.texts_to_sequences(english_sentences)\n",
    "decoder_sequences_input = decoder_tokenizer.texts_to_sequences(spanish_input_sentences)\n",
    "decoder_sequences_target = decoder_tokenizer.texts_to_sequences(spanish_output_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_ENCODER_SEQUENCE_LENGTH = max([len(sequence) for sequence in encoder_sequences])\n",
    "MAX_DECODER_SEQUENCE_LENGTH = max([len(sequence) for sequence in decoder_sequences_input])\n",
    "MAX_ENCODER_SEQUENCE_LENGTH = min(MAX_ENCODER_SEQUENCE_LENGTH, MAX_SEQUENCE_LENGTH)\n",
    "MAX_DECODER_SEQUENCE_LENGTH = min(MAX_DECODER_SEQUENCE_LENGTH, MAX_SEQUENCE_LENGTH)\n",
    "MAX_ENCODER_SEQUENCE_LENGTH, MAX_DECODER_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_sequences_padded = pad_sequences(encoder_sequences, maxlen=MAX_ENCODER_SEQUENCE_LENGTH, padding='post')\n",
    "decoder_sequences_input_padded = pad_sequences(decoder_sequences_input, maxlen=MAX_DECODER_SEQUENCE_LENGTH, padding='post')\n",
    "decoder_sequences_target_padded = pad_sequences(decoder_sequences_target, maxlen=MAX_DECODER_SEQUENCE_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2365, 5604)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We need to add + 1 so as to take into account the index 0.\n",
    "#In the output of the decoder we normally create a dense layer of size vocabulary_length taking for granted that\n",
    "#index 0 is a valid index and part of our vocabulary. Keras tokenizer doesn't take the zero into account and we \n",
    "#need to correct for it. We add a 1 to our vocabulary_length so the final dense layer is size vocabulary_length + 1.\n",
    "MAX_ENCODER_VOCABULARY = min(len(encoder_tokenizer.word_index), MAX_VOCABULARY) + 1\n",
    "MAX_DECODER_VOCABULARY = min(len(decoder_tokenizer.word_index), MAX_VOCABULARY) + 1\n",
    "MAX_ENCODER_VOCABULARY, MAX_DECODER_VOCABULARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_word2idx = {}\n",
    "encoder_idx2word = {}\n",
    "decoder_word2idx = {}\n",
    "decoder_idx2word = {}\n",
    "\n",
    "for word, index in encoder_tokenizer.word_index.items():\n",
    "    if index <= MAX_ENCODER_VOCABULARY:\n",
    "        encoder_word2idx[word] = index;\n",
    "        encoder_idx2word[index] = word;\n",
    "        \n",
    "for word, index in decoder_tokenizer.word_index.items():\n",
    "    if index <= MAX_DECODER_VOCABULARY:\n",
    "        decoder_word2idx[word] = index;\n",
    "        decoder_idx2word[index] = word;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot targets\n",
    "one_hot_targets = np.zeros((len(decoder_sequences_target_padded), MAX_DECODER_SEQUENCE_LENGTH, MAX_DECODER_VOCABULARY))\n",
    "for sequence_pos, sequence in enumerate(decoder_sequences_target_padded):\n",
    "    for word_pos, word_idx in enumerate(sequence):\n",
    "        one_hot_targets[sequence_pos, word_pos, word_idx] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#100 vector dimension.\n",
    "word2vec = {}\n",
    "with open(\"../basic_language_models/large_files/glove.6B/glove.6B.100d.txt\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        tokens = line.split();\n",
    "        word = tokens[0];\n",
    "        vector = np.array(tokens[1:])\n",
    "        word2vec[word] = vector;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_embeddings_weights = np.zeros((MAX_ENCODER_VOCABULARY, EMBEDDINGS_DIM));\n",
    "for word, index in encoder_tokenizer.word_index.items():\n",
    "    if index <= MAX_ENCODER_VOCABULARY:\n",
    "        if word in word2vec:\n",
    "            encoder_embeddings_weights[index] = word2vec[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_over_time(x):\n",
    "    array_norm = K.exp(x - K.max(x, axis = 1, keepdims = True))\n",
    "    return array_norm/K.sum(array_norm, axis = 1, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_embeddings = Embedding(input_dim = MAX_ENCODER_VOCABULARY, \n",
    "                        output_dim = EMBEDDINGS_DIM,\n",
    "                        weights = [encoder_embeddings_weights],\n",
    "                        input_length = MAX_ENCODER_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_embeddings = Embedding(input_dim = MAX_DECODER_VOCABULARY,\n",
    "                              output_dim = EMBEDDINGS_DIM)\n",
    "                              #weights = [encoder_embeddings_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def foo(module, x): pass <unknown> exec 1024\n",
      "def foo(module, x): pass <unknown> exec 1024\n"
     ]
    }
   ],
   "source": [
    "encoder_input = Input([MAX_ENCODER_SEQUENCE_LENGTH])\n",
    "encoder_input_embedded = encoder_embeddings(encoder_input)\n",
    "lstm = LSTM(LATENT_ENCODER_DIM, return_sequences=True)\n",
    "encoder_rnn = Bidirectional(lstm)(encoder_input_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'bidirectional_1/concat_2:0' shape=(?, 5, 400) dtype=float32>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden1 = 10\n",
    "repeat_layer = RepeatVector(MAX_ENCODER_SEQUENCE_LENGTH)\n",
    "concatenate_layer = Concatenate()\n",
    "dense1_layer = Dense(n_hidden1, activation='tanh')\n",
    "dense2_layer = Dense(1, activation = softmax_over_time)\n",
    "dot_layer = Dot(axes= 1)\n",
    "\n",
    "def attention_layer(h, rnn_output):\n",
    "    # rnn_output is the output of the bidirectional layer (?, Tx, 2*LATENT_ENCODER_DIM)\n",
    "    # h is the last hidden state of the decoder lstm (?, LATENT_DECODER_DIM)\n",
    "\n",
    "    h_T = repeat_layer(h) #(?, Tx, LATENT_DECODER_DIM)\n",
    "    h_encoder_concat = concatenate_layer([h_T, rnn_output])  #(?, Tx, LATENT_DECODER_DIM), (?, Tx, 2*LATENT_ENCODER_DIM)\n",
    "    #(?, T, LATENT_DECODER_DIM + 2*LATENT_ENCODER_DIM) ---> #(?, T, LATENT_DECODER_DIM + 2*LATENT_ENCODER_DIM)\n",
    "    \n",
    "    hidden1 = dense1_layer(h_encoder_concat)\n",
    "    #print(hidden1)  #(?, Tx, n_hidden1)\n",
    "    alphas = dense2_layer(hidden1)\n",
    "    #print(alphas) #(?, Tx, 1)\n",
    "    context = dot_layer([alphas, rnn_output])\n",
    "    return alphas, context; #(?, 1, 2*LATENT_ENCODER_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_h = Input([LATENT_DECODER_DIM])\n",
    "#attention_layer(input_h)\n",
    "def permute_dimensions(x):\n",
    "    #Stacking and permuting Must be inside a single function so they are passed together to the Lambda function\n",
    "    #If done independently keras will complain that stack or permute_dimentions are not layers.\n",
    "    x = K.stack(x)\n",
    "    return K.permute_dimensions(x, (1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_DECODER_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Input([MAX_DECODER_SEQUENCE_LENGTH], name= \"decoder_input_\")\n",
    "decoder_input_embedded = decoder_embeddings(decoder_input)\n",
    "\n",
    "lstm_decoder = LSTM(units = LATENT_DECODER_DIM, return_state=True);\n",
    "\n",
    "dense_decoder = Dense(MAX_DECODER_VOCABULARY, activation = 'softmax');\n",
    "concatenate_context_and_input = Concatenate(axis = 2)\n",
    "h_init = Input([LATENT_DECODER_DIM], name = \"h_init\")\n",
    "c_init = Input([LATENT_DECODER_DIM], name = \"c_init\")\n",
    "h = h_init;\n",
    "c = c_init;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "\n",
    "for t in range(MAX_DECODER_SEQUENCE_LENGTH):\n",
    "    selector = Lambda(lambda x: x[:, t: t + 1])    \n",
    "    current_input = selector(decoder_input_embedded)  #(?, 1, EMBEDDIGS_DIM)\n",
    "    __, context = attention_layer(h, encoder_rnn) #(?, 1, 2*LATENT_ENCODER_DIM)    \n",
    "    #Teacher forcing\n",
    "\n",
    "    context_concatenated = concatenate_context_and_input([context, current_input]) #(?, 1, EMBEDDINGS_DIM + 2*LATENT_ENCODER_DIM)\n",
    "    o, h, c = lstm_decoder(context_concatenated, initial_state = [h, c])\n",
    "    word_probs = dense_decoder(o) #(?, MAX_DECODER_VOCABULARY)\n",
    "    outputs.append(word_probs)\n",
    "    \n",
    "    \n",
    "#outputs is a list of Ty elements each of size batch_size, MAX_DECODER_VOCABULARY\n",
    " #(Ty, ?, MAX_DECODER_VOCABULARY)\n",
    "outputs_layer = Lambda(permute_dimensions)(outputs)  #(?, Ty, MAX_DECODER_VOCABULARY)\n",
    "    \n",
    "#model = Model(inputs = [encoder_input, decoder_input, h_init, c_init], outputs = outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs = [encoder_input, decoder_input, h_init, c_init], outputs = outputs_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(encoder_sequences_padded)\n",
    "h_init_ = np.zeros((m, LATENT_DECODER_DIM))\n",
    "c_init_ = np.zeros((m, LATENT_DECODER_DIM))\n",
    "model.compile(optimizer = Adam(0.001), \n",
    "              loss = 'categorical_crossentropy', \n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 4.3010 - acc: 0.5710 - val_loss: 3.3599 - val_acc: 0.5258\n",
      "Epoch 2/200\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 2.7631 - acc: 0.5807 - val_loss: 3.0868 - val_acc: 0.5258\n",
      "Epoch 3/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 2.5181 - acc: 0.5961 - val_loss: 2.9332 - val_acc: 0.6048\n",
      "Epoch 4/200\n",
      "8000/8000 [==============================] - 44s 6ms/step - loss: 2.3827 - acc: 0.6807 - val_loss: 2.8390 - val_acc: 0.6354\n",
      "Epoch 5/200\n",
      "8000/8000 [==============================] - 44s 6ms/step - loss: 2.2946 - acc: 0.6917 - val_loss: 2.7922 - val_acc: 0.6367\n",
      "Epoch 6/200\n",
      "8000/8000 [==============================] - 44s 6ms/step - loss: 2.2328 - acc: 0.6932 - val_loss: 2.7418 - val_acc: 0.6369\n",
      "Epoch 7/200\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 2.1827 - acc: 0.6981 - val_loss: 2.7113 - val_acc: 0.6501\n",
      "Epoch 8/200\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 2.1444 - acc: 0.7038 - val_loss: 2.6930 - val_acc: 0.6583\n",
      "Epoch 9/200\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 2.1125 - acc: 0.7055 - val_loss: 2.6746 - val_acc: 0.6596\n",
      "Epoch 10/200\n",
      "8000/8000 [==============================] - 44s 6ms/step - loss: 2.0860 - acc: 0.7057 - val_loss: 2.6486 - val_acc: 0.6598\n",
      "Epoch 11/200\n",
      "8000/8000 [==============================] - 45s 6ms/step - loss: 2.0610 - acc: 0.7062 - val_loss: 2.6248 - val_acc: 0.6600\n",
      "Epoch 12/200\n",
      "8000/8000 [==============================] - 44s 5ms/step - loss: 2.0335 - acc: 0.7067 - val_loss: 2.6077 - val_acc: 0.6615\n",
      "Epoch 13/200\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 2.0032 - acc: 0.7083 - val_loss: 2.5798 - val_acc: 0.6636\n",
      "Epoch 14/200\n",
      "8000/8000 [==============================] - 44s 5ms/step - loss: 1.9659 - acc: 0.7119 - val_loss: 2.5336 - val_acc: 0.6675\n",
      "Epoch 15/200\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 1.9243 - acc: 0.7165 - val_loss: 2.4911 - val_acc: 0.6677\n",
      "Epoch 16/200\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 1.8825 - acc: 0.7213 - val_loss: 2.4552 - val_acc: 0.6736\n",
      "Epoch 17/200\n",
      "8000/8000 [==============================] - 45s 6ms/step - loss: 1.8431 - acc: 0.7236 - val_loss: 2.4222 - val_acc: 0.6738\n",
      "Epoch 18/200\n",
      "8000/8000 [==============================] - 50s 6ms/step - loss: 1.8050 - acc: 0.7253 - val_loss: 2.3957 - val_acc: 0.6763\n",
      "Epoch 19/200\n",
      "8000/8000 [==============================] - 49s 6ms/step - loss: 1.7701 - acc: 0.7287 - val_loss: 2.3672 - val_acc: 0.6775\n",
      "Epoch 20/200\n",
      "8000/8000 [==============================] - 51s 6ms/step - loss: 1.7361 - acc: 0.7307 - val_loss: 2.3658 - val_acc: 0.6779\n",
      "Epoch 21/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 1.7030 - acc: 0.7334 - val_loss: 2.3295 - val_acc: 0.6797\n",
      "Epoch 22/200\n",
      "8000/8000 [==============================] - 44s 6ms/step - loss: 1.6683 - acc: 0.7373 - val_loss: 2.3049 - val_acc: 0.6815\n",
      "Epoch 23/200\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 1.6346 - acc: 0.7410 - val_loss: 2.2868 - val_acc: 0.6831\n",
      "Epoch 24/200\n",
      "8000/8000 [==============================] - 48s 6ms/step - loss: 1.6013 - acc: 0.7441 - val_loss: 2.2719 - val_acc: 0.6852\n",
      "Epoch 25/200\n",
      "8000/8000 [==============================] - 45s 6ms/step - loss: 1.5688 - acc: 0.7481 - val_loss: 2.2553 - val_acc: 0.6869\n",
      "Epoch 26/200\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 1.5371 - acc: 0.7510 - val_loss: 2.2397 - val_acc: 0.6908\n",
      "Epoch 27/200\n",
      "8000/8000 [==============================] - 44s 5ms/step - loss: 1.5067 - acc: 0.7538 - val_loss: 2.2201 - val_acc: 0.6917\n",
      "Epoch 28/200\n",
      "8000/8000 [==============================] - 44s 5ms/step - loss: 1.4759 - acc: 0.7572 - val_loss: 2.2103 - val_acc: 0.6922\n",
      "Epoch 29/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 1.4453 - acc: 0.7598 - val_loss: 2.1956 - val_acc: 0.6946\n",
      "Epoch 30/200\n",
      "8000/8000 [==============================] - 53s 7ms/step - loss: 1.4156 - acc: 0.7631 - val_loss: 2.1879 - val_acc: 0.6973\n",
      "Epoch 31/200\n",
      "8000/8000 [==============================] - 48s 6ms/step - loss: 1.3865 - acc: 0.7661 - val_loss: 2.1723 - val_acc: 0.6987\n",
      "Epoch 32/200\n",
      "8000/8000 [==============================] - 48s 6ms/step - loss: 1.3577 - acc: 0.7694 - val_loss: 2.1530 - val_acc: 0.7041\n",
      "Epoch 33/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 1.3303 - acc: 0.7724 - val_loss: 2.1491 - val_acc: 0.7033\n",
      "Epoch 34/200\n",
      "8000/8000 [==============================] - 50s 6ms/step - loss: 1.3023 - acc: 0.7755 - val_loss: 2.1318 - val_acc: 0.7043\n",
      "Epoch 35/200\n",
      "8000/8000 [==============================] - 49s 6ms/step - loss: 1.2740 - acc: 0.7792 - val_loss: 2.1233 - val_acc: 0.7077\n",
      "Epoch 36/200\n",
      "8000/8000 [==============================] - 48s 6ms/step - loss: 1.2463 - acc: 0.7837 - val_loss: 2.1176 - val_acc: 0.7099\n",
      "Epoch 37/200\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 1.2192 - acc: 0.7862 - val_loss: 2.1051 - val_acc: 0.7126\n",
      "Epoch 38/200\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 1.1931 - acc: 0.7897 - val_loss: 2.1008 - val_acc: 0.7141\n",
      "Epoch 39/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 1.1664 - acc: 0.7926 - val_loss: 2.0915 - val_acc: 0.7172\n",
      "Epoch 40/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 1.1400 - acc: 0.7958 - val_loss: 2.0784 - val_acc: 0.7184\n",
      "Epoch 41/200\n",
      "8000/8000 [==============================] - 48s 6ms/step - loss: 1.1144 - acc: 0.7990 - val_loss: 2.0724 - val_acc: 0.7193\n",
      "Epoch 42/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 1.0884 - acc: 0.8024 - val_loss: 2.0660 - val_acc: 0.7227\n",
      "Epoch 43/200\n",
      "8000/8000 [==============================] - 49s 6ms/step - loss: 1.0646 - acc: 0.8048 - val_loss: 2.0540 - val_acc: 0.7239\n",
      "Epoch 44/200\n",
      "8000/8000 [==============================] - 49s 6ms/step - loss: 1.0396 - acc: 0.8078 - val_loss: 2.0500 - val_acc: 0.7232\n",
      "Epoch 45/200\n",
      "8000/8000 [==============================] - 49s 6ms/step - loss: 1.0143 - acc: 0.8109 - val_loss: 2.0498 - val_acc: 0.7258\n",
      "Epoch 46/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.9896 - acc: 0.8139 - val_loss: 2.0356 - val_acc: 0.7274\n",
      "Epoch 47/200\n",
      "8000/8000 [==============================] - 50s 6ms/step - loss: 0.9648 - acc: 0.8179 - val_loss: 2.0322 - val_acc: 0.7290\n",
      "Epoch 48/200\n",
      "8000/8000 [==============================] - 48s 6ms/step - loss: 0.9400 - acc: 0.8207 - val_loss: 2.0261 - val_acc: 0.7283\n",
      "Epoch 49/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.9159 - acc: 0.8236 - val_loss: 2.0229 - val_acc: 0.7293\n",
      "Epoch 50/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.8916 - acc: 0.8271 - val_loss: 2.0152 - val_acc: 0.7296\n",
      "Epoch 51/200\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 0.8675 - acc: 0.8306 - val_loss: 2.0129 - val_acc: 0.7313\n",
      "Epoch 52/200\n",
      "8000/8000 [==============================] - 48s 6ms/step - loss: 0.8417 - acc: 0.8334 - val_loss: 2.0099 - val_acc: 0.7318\n",
      "Epoch 53/200\n",
      "8000/8000 [==============================] - 45s 6ms/step - loss: 0.8164 - acc: 0.8372 - val_loss: 2.0064 - val_acc: 0.7328\n",
      "Epoch 54/200\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 0.7918 - acc: 0.8407 - val_loss: 2.0009 - val_acc: 0.7346\n",
      "Epoch 55/200\n",
      "8000/8000 [==============================] - 45s 6ms/step - loss: 0.7674 - acc: 0.8440 - val_loss: 2.0001 - val_acc: 0.7371\n",
      "Epoch 56/200\n",
      "8000/8000 [==============================] - 49s 6ms/step - loss: 0.7425 - acc: 0.8481 - val_loss: 2.0009 - val_acc: 0.7349\n",
      "Epoch 57/200\n",
      "8000/8000 [==============================] - 48s 6ms/step - loss: 0.7186 - acc: 0.8515 - val_loss: 1.9927 - val_acc: 0.7386\n",
      "Epoch 58/200\n",
      "8000/8000 [==============================] - 49s 6ms/step - loss: 0.6947 - acc: 0.8558 - val_loss: 1.9901 - val_acc: 0.7373\n",
      "Epoch 59/200\n",
      "8000/8000 [==============================] - 48s 6ms/step - loss: 0.6723 - acc: 0.8593 - val_loss: 1.9904 - val_acc: 0.7401\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 49s 6ms/step - loss: 0.6480 - acc: 0.8643 - val_loss: 1.9874 - val_acc: 0.7401\n",
      "Epoch 61/200\n",
      "8000/8000 [==============================] - 50s 6ms/step - loss: 0.6237 - acc: 0.8694 - val_loss: 1.9874 - val_acc: 0.7411\n",
      "Epoch 62/200\n",
      "8000/8000 [==============================] - 50s 6ms/step - loss: 0.6009 - acc: 0.8740 - val_loss: 1.9830 - val_acc: 0.7424\n",
      "Epoch 63/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.5782 - acc: 0.8780 - val_loss: 1.9810 - val_acc: 0.7422\n",
      "Epoch 64/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.5551 - acc: 0.8826 - val_loss: 1.9780 - val_acc: 0.7434\n",
      "Epoch 65/200\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 0.5323 - acc: 0.8887 - val_loss: 1.9777 - val_acc: 0.7426\n",
      "Epoch 66/200\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 0.5115 - acc: 0.8924 - val_loss: 1.9774 - val_acc: 0.7434\n",
      "Epoch 67/200\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 0.4894 - acc: 0.8974 - val_loss: 1.9749 - val_acc: 0.7451\n",
      "Epoch 68/200\n",
      "8000/8000 [==============================] - 45s 6ms/step - loss: 0.4683 - acc: 0.9014 - val_loss: 1.9747 - val_acc: 0.7448\n",
      "Epoch 69/200\n",
      "8000/8000 [==============================] - 44s 6ms/step - loss: 0.4484 - acc: 0.9060 - val_loss: 1.9753 - val_acc: 0.7447\n",
      "Epoch 70/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.4282 - acc: 0.9108 - val_loss: 1.9739 - val_acc: 0.7451\n",
      "Epoch 71/200\n",
      "8000/8000 [==============================] - 48s 6ms/step - loss: 0.4082 - acc: 0.9150 - val_loss: 1.9732 - val_acc: 0.7454\n",
      "Epoch 72/200\n",
      "8000/8000 [==============================] - 49s 6ms/step - loss: 0.3904 - acc: 0.9196 - val_loss: 1.9676 - val_acc: 0.7459\n",
      "Epoch 73/200\n",
      "8000/8000 [==============================] - 44s 5ms/step - loss: 0.3730 - acc: 0.9225 - val_loss: 1.9683 - val_acc: 0.7462\n",
      "Epoch 74/200\n",
      "8000/8000 [==============================] - 44s 5ms/step - loss: 0.3556 - acc: 0.9252 - val_loss: 1.9747 - val_acc: 0.7454\n",
      "Epoch 75/200\n",
      "8000/8000 [==============================] - 50s 6ms/step - loss: 0.3399 - acc: 0.9283 - val_loss: 1.9684 - val_acc: 0.7476\n",
      "Epoch 76/200\n",
      "8000/8000 [==============================] - 45s 6ms/step - loss: 0.3248 - acc: 0.9317 - val_loss: 1.9681 - val_acc: 0.7492\n",
      "Epoch 77/200\n",
      "8000/8000 [==============================] - 44s 5ms/step - loss: 0.3100 - acc: 0.9338 - val_loss: 1.9693 - val_acc: 0.7491\n",
      "Epoch 78/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.2963 - acc: 0.9356 - val_loss: 1.9714 - val_acc: 0.7472\n",
      "Epoch 79/200\n",
      "8000/8000 [==============================] - 49s 6ms/step - loss: 0.2835 - acc: 0.9384 - val_loss: 1.9771 - val_acc: 0.7485\n",
      "Epoch 80/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.2718 - acc: 0.9394 - val_loss: 1.9784 - val_acc: 0.7507\n",
      "Epoch 81/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.2593 - acc: 0.9414 - val_loss: 1.9776 - val_acc: 0.7488\n",
      "Epoch 82/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.2493 - acc: 0.9431 - val_loss: 1.9849 - val_acc: 0.7479\n",
      "Epoch 83/200\n",
      "8000/8000 [==============================] - 51s 6ms/step - loss: 0.2384 - acc: 0.9444 - val_loss: 1.9789 - val_acc: 0.7504\n",
      "Epoch 84/200\n",
      "8000/8000 [==============================] - 48s 6ms/step - loss: 0.2287 - acc: 0.9457 - val_loss: 1.9824 - val_acc: 0.7514\n",
      "Epoch 85/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.2195 - acc: 0.9468 - val_loss: 1.9888 - val_acc: 0.7503\n",
      "Epoch 86/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.2109 - acc: 0.9483 - val_loss: 1.9892 - val_acc: 0.7501\n",
      "Epoch 87/200\n",
      "8000/8000 [==============================] - 51s 6ms/step - loss: 0.2029 - acc: 0.9486 - val_loss: 1.9892 - val_acc: 0.7511\n",
      "Epoch 88/200\n",
      "8000/8000 [==============================] - 51s 6ms/step - loss: 0.1952 - acc: 0.9494 - val_loss: 1.9956 - val_acc: 0.7524\n",
      "Epoch 89/200\n",
      "8000/8000 [==============================] - 51s 6ms/step - loss: 0.1885 - acc: 0.9502 - val_loss: 1.9916 - val_acc: 0.7519\n",
      "Epoch 90/200\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 0.1817 - acc: 0.9513 - val_loss: 2.0008 - val_acc: 0.7503\n",
      "Epoch 91/200\n",
      "8000/8000 [==============================] - 48s 6ms/step - loss: 0.1756 - acc: 0.9514 - val_loss: 2.0011 - val_acc: 0.7517\n",
      "Epoch 92/200\n",
      "8000/8000 [==============================] - 48s 6ms/step - loss: 0.1703 - acc: 0.9523 - val_loss: 2.0038 - val_acc: 0.7527\n",
      "Epoch 93/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.1656 - acc: 0.9528 - val_loss: 2.0036 - val_acc: 0.7525\n",
      "Epoch 94/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.1600 - acc: 0.9536 - val_loss: 2.0079 - val_acc: 0.7512\n",
      "Epoch 95/200\n",
      "8000/8000 [==============================] - 48s 6ms/step - loss: 0.1545 - acc: 0.9542 - val_loss: 2.0078 - val_acc: 0.7527\n",
      "Epoch 96/200\n",
      "8000/8000 [==============================] - 48s 6ms/step - loss: 0.1503 - acc: 0.9545 - val_loss: 2.0151 - val_acc: 0.7527\n",
      "Epoch 97/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.1475 - acc: 0.9543 - val_loss: 2.0179 - val_acc: 0.7517\n",
      "Epoch 98/200\n",
      "8000/8000 [==============================] - 48s 6ms/step - loss: 0.1439 - acc: 0.9544 - val_loss: 2.0182 - val_acc: 0.7522\n",
      "Epoch 99/200\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 0.1393 - acc: 0.9553 - val_loss: 2.0252 - val_acc: 0.7525\n",
      "Epoch 100/200\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 0.1352 - acc: 0.9550 - val_loss: 2.0245 - val_acc: 0.7525\n",
      "Epoch 101/200\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.1311 - acc: 0.9562 - val_loss: 2.0277 - val_acc: 0.7519\n",
      "Epoch 102/200\n",
      "8000/8000 [==============================] - 44s 6ms/step - loss: 0.1279 - acc: 0.9563 - val_loss: 2.0320 - val_acc: 0.7519\n",
      "Epoch 103/200\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 0.1249 - acc: 0.9563 - val_loss: 2.0350 - val_acc: 0.7531\n",
      "Epoch 104/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.1224 - acc: 0.9562 - val_loss: 2.0403 - val_acc: 0.7539\n",
      "Epoch 105/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.1198 - acc: 0.9566 - val_loss: 2.0373 - val_acc: 0.7531\n",
      "Epoch 106/200\n",
      "8000/8000 [==============================] - 50s 6ms/step - loss: 0.1171 - acc: 0.9566 - val_loss: 2.0467 - val_acc: 0.7522\n",
      "Epoch 107/200\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 0.1152 - acc: 0.9565 - val_loss: 2.0457 - val_acc: 0.7526\n",
      "Epoch 108/200\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 0.1127 - acc: 0.9569 - val_loss: 2.0503 - val_acc: 0.7519\n",
      "Epoch 109/200\n",
      "8000/8000 [==============================] - 44s 6ms/step - loss: 0.1106 - acc: 0.9571 - val_loss: 2.0519 - val_acc: 0.7538\n",
      "Epoch 110/200\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.1088 - acc: 0.9572 - val_loss: 2.0559 - val_acc: 0.7533\n",
      "Epoch 111/200\n",
      "8000/8000 [==============================] - 44s 5ms/step - loss: 0.1076 - acc: 0.9573 - val_loss: 2.0550 - val_acc: 0.7525\n",
      "Epoch 112/200\n",
      "8000/8000 [==============================] - 44s 5ms/step - loss: 0.1062 - acc: 0.9571 - val_loss: 2.0534 - val_acc: 0.7547\n",
      "Epoch 113/200\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.1040 - acc: 0.9570 - val_loss: 2.0624 - val_acc: 0.7532\n",
      "Epoch 114/200\n",
      "8000/8000 [==============================] - 45s 6ms/step - loss: 0.1019 - acc: 0.9573 - val_loss: 2.0640 - val_acc: 0.7537\n",
      "Epoch 115/200\n",
      "8000/8000 [==============================] - 49s 6ms/step - loss: 0.1002 - acc: 0.9572 - val_loss: 2.0691 - val_acc: 0.7543\n",
      "Epoch 116/200\n",
      "8000/8000 [==============================] - 45s 6ms/step - loss: 0.0990 - acc: 0.9574 - val_loss: 2.0697 - val_acc: 0.7529\n",
      "Epoch 117/200\n",
      "8000/8000 [==============================] - 44s 5ms/step - loss: 0.0971 - acc: 0.9574 - val_loss: 2.0750 - val_acc: 0.7543\n",
      "Epoch 118/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.0961 - acc: 0.9573 - val_loss: 2.0763 - val_acc: 0.7541\n",
      "Epoch 119/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.0949 - acc: 0.9573 - val_loss: 2.0829 - val_acc: 0.7544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "8000/8000 [==============================] - 48s 6ms/step - loss: 0.0935 - acc: 0.9574 - val_loss: 2.0841 - val_acc: 0.7541\n",
      "Epoch 121/200\n",
      "8000/8000 [==============================] - 48s 6ms/step - loss: 0.0927 - acc: 0.9573 - val_loss: 2.0919 - val_acc: 0.7545\n",
      "Epoch 122/200\n",
      "8000/8000 [==============================] - 48s 6ms/step - loss: 0.0916 - acc: 0.9572 - val_loss: 2.0979 - val_acc: 0.7539\n",
      "Epoch 123/200\n",
      "8000/8000 [==============================] - 48s 6ms/step - loss: 0.0908 - acc: 0.9570 - val_loss: 2.0931 - val_acc: 0.7541\n",
      "Epoch 124/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.0897 - acc: 0.9575 - val_loss: 2.0921 - val_acc: 0.7534\n",
      "Epoch 125/200\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 0.0893 - acc: 0.9573 - val_loss: 2.0944 - val_acc: 0.7536\n",
      "Epoch 126/200\n",
      "8000/8000 [==============================] - 48s 6ms/step - loss: 0.0900 - acc: 0.9573 - val_loss: 2.0991 - val_acc: 0.7556\n",
      "Epoch 127/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.0882 - acc: 0.9571 - val_loss: 2.0972 - val_acc: 0.7547\n",
      "Epoch 128/200\n",
      "8000/8000 [==============================] - 49s 6ms/step - loss: 0.0868 - acc: 0.9574 - val_loss: 2.1038 - val_acc: 0.7536\n",
      "Epoch 129/200\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 0.0852 - acc: 0.9575 - val_loss: 2.1069 - val_acc: 0.7547\n",
      "Epoch 130/200\n",
      "8000/8000 [==============================] - 48s 6ms/step - loss: 0.0842 - acc: 0.9580 - val_loss: 2.1112 - val_acc: 0.7534\n",
      "Epoch 131/200\n",
      "8000/8000 [==============================] - 48s 6ms/step - loss: 0.0835 - acc: 0.9573 - val_loss: 2.1177 - val_acc: 0.7532\n",
      "Epoch 132/200\n",
      "8000/8000 [==============================] - 49s 6ms/step - loss: 0.0827 - acc: 0.9573 - val_loss: 2.1137 - val_acc: 0.7540\n",
      "Epoch 133/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.0822 - acc: 0.9571 - val_loss: 2.1189 - val_acc: 0.7527\n",
      "Epoch 134/200\n",
      "8000/8000 [==============================] - 48s 6ms/step - loss: 0.0821 - acc: 0.9574 - val_loss: 2.1204 - val_acc: 0.7531\n",
      "Epoch 135/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.0811 - acc: 0.9578 - val_loss: 2.1224 - val_acc: 0.7523\n",
      "Epoch 136/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.0802 - acc: 0.9580 - val_loss: 2.1263 - val_acc: 0.7537\n",
      "Epoch 137/200\n",
      "8000/8000 [==============================] - 48s 6ms/step - loss: 0.0795 - acc: 0.9577 - val_loss: 2.1286 - val_acc: 0.7544\n",
      "Epoch 138/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.0786 - acc: 0.9580 - val_loss: 2.1299 - val_acc: 0.7551\n",
      "Epoch 139/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.0784 - acc: 0.9575 - val_loss: 2.1359 - val_acc: 0.7537\n",
      "Epoch 140/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.0780 - acc: 0.9569 - val_loss: 2.1349 - val_acc: 0.7534\n",
      "Epoch 141/200\n",
      "8000/8000 [==============================] - 44s 5ms/step - loss: 0.0778 - acc: 0.9575 - val_loss: 2.1417 - val_acc: 0.7539\n",
      "Epoch 142/200\n",
      "8000/8000 [==============================] - 44s 5ms/step - loss: 0.0775 - acc: 0.9577 - val_loss: 2.1460 - val_acc: 0.7530\n",
      "Epoch 143/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.0767 - acc: 0.9574 - val_loss: 2.1467 - val_acc: 0.7552\n",
      "Epoch 144/200\n",
      "8000/8000 [==============================] - 45s 6ms/step - loss: 0.0763 - acc: 0.9575 - val_loss: 2.1446 - val_acc: 0.7551\n",
      "Epoch 145/200\n",
      "8000/8000 [==============================] - 44s 6ms/step - loss: 0.0759 - acc: 0.9576 - val_loss: 2.1470 - val_acc: 0.7532\n",
      "Epoch 146/200\n",
      "8000/8000 [==============================] - 44s 6ms/step - loss: 0.0755 - acc: 0.9569 - val_loss: 2.1505 - val_acc: 0.7527\n",
      "Epoch 147/200\n",
      "8000/8000 [==============================] - 45s 6ms/step - loss: 0.0751 - acc: 0.9571 - val_loss: 2.1535 - val_acc: 0.7538\n",
      "Epoch 148/200\n",
      "8000/8000 [==============================] - 44s 6ms/step - loss: 0.0745 - acc: 0.9570 - val_loss: 2.1504 - val_acc: 0.7558\n",
      "Epoch 149/200\n",
      "8000/8000 [==============================] - 45s 6ms/step - loss: 0.0744 - acc: 0.9571 - val_loss: 2.1687 - val_acc: 0.7539\n",
      "Epoch 150/200\n",
      "8000/8000 [==============================] - 45s 6ms/step - loss: 0.0742 - acc: 0.9576 - val_loss: 2.1527 - val_acc: 0.7551\n",
      "Epoch 151/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.0765 - acc: 0.9572 - val_loss: 2.1606 - val_acc: 0.7542\n",
      "Epoch 152/200\n",
      "8000/8000 [==============================] - 48s 6ms/step - loss: 0.0784 - acc: 0.9572 - val_loss: 2.1630 - val_acc: 0.7541\n",
      "Epoch 153/200\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 0.0767 - acc: 0.9570 - val_loss: 2.1503 - val_acc: 0.7538\n",
      "Epoch 154/200\n",
      "8000/8000 [==============================] - 49s 6ms/step - loss: 0.0767 - acc: 0.9567 - val_loss: 2.1597 - val_acc: 0.7553\n",
      "Epoch 155/200\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 0.0740 - acc: 0.9575 - val_loss: 2.1628 - val_acc: 0.7550\n",
      "Epoch 156/200\n",
      "8000/8000 [==============================] - 50s 6ms/step - loss: 0.0723 - acc: 0.9578 - val_loss: 2.1659 - val_acc: 0.7553\n",
      "Epoch 157/200\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 0.0720 - acc: 0.9574 - val_loss: 2.1702 - val_acc: 0.7545\n",
      "Epoch 158/200\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 0.0714 - acc: 0.9575 - val_loss: 2.1682 - val_acc: 0.7558\n",
      "Epoch 159/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.0710 - acc: 0.9573 - val_loss: 2.1722 - val_acc: 0.7567\n",
      "Epoch 160/200\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 0.0706 - acc: 0.9578 - val_loss: 2.1692 - val_acc: 0.7556\n",
      "Epoch 161/200\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 0.0704 - acc: 0.9573 - val_loss: 2.1775 - val_acc: 0.7543\n",
      "Epoch 162/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.0701 - acc: 0.9566 - val_loss: 2.1770 - val_acc: 0.7561\n",
      "Epoch 163/200\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 0.0700 - acc: 0.9577 - val_loss: 2.1781 - val_acc: 0.7551\n",
      "Epoch 164/200\n",
      "8000/8000 [==============================] - 48s 6ms/step - loss: 0.0698 - acc: 0.9575 - val_loss: 2.1773 - val_acc: 0.7565\n",
      "Epoch 165/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.0697 - acc: 0.9577 - val_loss: 2.1931 - val_acc: 0.7552\n",
      "Epoch 166/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.0695 - acc: 0.9570 - val_loss: 2.1860 - val_acc: 0.7559\n",
      "Epoch 167/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.0691 - acc: 0.9572 - val_loss: 2.1932 - val_acc: 0.7552\n",
      "Epoch 168/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.0690 - acc: 0.9572 - val_loss: 2.1973 - val_acc: 0.7553\n",
      "Epoch 169/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.0690 - acc: 0.9568 - val_loss: 2.1881 - val_acc: 0.7547\n",
      "Epoch 170/200\n",
      "8000/8000 [==============================] - 44s 5ms/step - loss: 0.0685 - acc: 0.9574 - val_loss: 2.1945 - val_acc: 0.7552\n",
      "Epoch 171/200\n",
      "8000/8000 [==============================] - 44s 5ms/step - loss: 0.0685 - acc: 0.9576 - val_loss: 2.1980 - val_acc: 0.7550\n",
      "Epoch 172/200\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.0683 - acc: 0.9568 - val_loss: 2.2000 - val_acc: 0.7555\n",
      "Epoch 173/200\n",
      "8000/8000 [==============================] - 44s 5ms/step - loss: 0.0682 - acc: 0.9567 - val_loss: 2.2029 - val_acc: 0.7550\n",
      "Epoch 174/200\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.0678 - acc: 0.9570 - val_loss: 2.2107 - val_acc: 0.7567\n",
      "Epoch 175/200\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.0678 - acc: 0.9574 - val_loss: 2.2076 - val_acc: 0.7552\n",
      "Epoch 176/200\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 0.0677 - acc: 0.9571 - val_loss: 2.2078 - val_acc: 0.7550\n",
      "Epoch 177/200\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.0687 - acc: 0.9573 - val_loss: 2.2254 - val_acc: 0.7549\n",
      "Epoch 178/200\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.0698 - acc: 0.9577 - val_loss: 2.2163 - val_acc: 0.7552\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 44s 5ms/step - loss: 0.0727 - acc: 0.9569 - val_loss: 2.2064 - val_acc: 0.7561\n",
      "Epoch 180/200\n",
      "8000/8000 [==============================] - 44s 6ms/step - loss: 0.0767 - acc: 0.9567 - val_loss: 2.2092 - val_acc: 0.7545\n",
      "Epoch 181/200\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 0.0746 - acc: 0.9568 - val_loss: 2.2067 - val_acc: 0.7539\n",
      "Epoch 182/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.0705 - acc: 0.9575 - val_loss: 2.2027 - val_acc: 0.7559\n",
      "Epoch 183/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.0683 - acc: 0.9578 - val_loss: 2.2118 - val_acc: 0.7554\n",
      "Epoch 184/200\n",
      "8000/8000 [==============================] - 49s 6ms/step - loss: 0.0672 - acc: 0.9574 - val_loss: 2.2148 - val_acc: 0.7562\n",
      "Epoch 185/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.0668 - acc: 0.9575 - val_loss: 2.2198 - val_acc: 0.7557\n",
      "Epoch 186/200\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.0664 - acc: 0.9576 - val_loss: 2.2174 - val_acc: 0.7559\n",
      "Epoch 187/200\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.0657 - acc: 0.9574 - val_loss: 2.2203 - val_acc: 0.7564\n",
      "Epoch 188/200\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.0656 - acc: 0.9575 - val_loss: 2.2213 - val_acc: 0.7560\n",
      "Epoch 189/200\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.0657 - acc: 0.9572 - val_loss: 2.2249 - val_acc: 0.7565\n",
      "Epoch 190/200\n",
      "8000/8000 [==============================] - 44s 5ms/step - loss: 0.0656 - acc: 0.9571 - val_loss: 2.2208 - val_acc: 0.7563\n",
      "Epoch 191/200\n",
      "8000/8000 [==============================] - 48s 6ms/step - loss: 0.0655 - acc: 0.9575 - val_loss: 2.2251 - val_acc: 0.7566\n",
      "Epoch 192/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.0653 - acc: 0.9570 - val_loss: 2.2316 - val_acc: 0.7553\n",
      "Epoch 193/200\n",
      "8000/8000 [==============================] - 45s 6ms/step - loss: 0.0652 - acc: 0.9573 - val_loss: 2.2306 - val_acc: 0.7544\n",
      "Epoch 194/200\n",
      "8000/8000 [==============================] - 44s 5ms/step - loss: 0.0650 - acc: 0.9576 - val_loss: 2.2330 - val_acc: 0.7554\n",
      "Epoch 195/200\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.0652 - acc: 0.9575 - val_loss: 2.2346 - val_acc: 0.7561\n",
      "Epoch 196/200\n",
      "8000/8000 [==============================] - 44s 5ms/step - loss: 0.0651 - acc: 0.9572 - val_loss: 2.2372 - val_acc: 0.7547\n",
      "Epoch 197/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.0649 - acc: 0.9576 - val_loss: 2.2363 - val_acc: 0.7564\n",
      "Epoch 198/200\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 0.0648 - acc: 0.9573 - val_loss: 2.2379 - val_acc: 0.7558\n",
      "Epoch 199/200\n",
      "8000/8000 [==============================] - 44s 6ms/step - loss: 0.0649 - acc: 0.9573 - val_loss: 2.2434 - val_acc: 0.7561\n",
      "Epoch 200/200\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.0648 - acc: 0.9571 - val_loss: 2.2406 - val_acc: 0.7571\n"
     ]
    }
   ],
   "source": [
    "\n",
    "r = model.fit(x = [encoder_sequences_padded, decoder_sequences_input_padded, h_init_, c_init_], \n",
    "          y = one_hot_targets, \n",
    "          batch_size = 128, \n",
    "          epochs = 200, \n",
    "          validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_weights(\"./19032019_weights_translator_last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(r.history['val_loss'], label = \"val loss\")\n",
    "plt.plot(r.history['loss'], label = \"train loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3Xd8VFX+//HXmcmUTHqFkAAJSG9SBHtZFRFXUVDBruviFvWr3/1+/em6RVa3uEW/rruuLq6uZbGtyAouiotSbEhRkBpKABMS0ntmMu38/rgTDCGBAMnczOTzfDzymJk7d+Z+cmbyzpkz956rtNYIIYSILhazCxBCCNH1JNyFECIKSbgLIUQUknAXQogoJOEuhBBRSMJdCCGikIS7EEJEIQl3IYSIQhLuQggRhWLM2nB6errOzc01a/NCCBGRNmzYUKG1zjjWeqaFe25uLuvXrzdr80IIEZGUUvs7s54MywghRBSScBdCiCgk4S6EEFHItDH39vh8PoqKivB4PGaXEnGcTic5OTnYbDazSxFC9AA9KtyLiopISEggNzcXpZTZ5UQMrTWVlZUUFRWRl5dndjlCiB6gRw3LeDwe0tLSJNiPk1KKtLQ0+cQjhDikR4U7IMF+gqTdhBCt9ahhGSEiUjAA7hoI+kEHAQ1aH+W6Dl0Ptn/d6gBHPNhc4K6G+hJjmS029OMCvwcayqDhIDSUGuulDYG0wUZN9jhQVmiqgMYKY92mCrDFGc/dWA7OZMgcAelDwdsA5fngqQFLDMSmGrUE/cbzZQyHhCyjTn+zsTzoh+p9xmMS+oEjwagvxmlcBgNQuhmq9oK3EZJyjLoCPmNdTy3UFkFCX6OO+D5w8CuoKQS7C3weiHFAn1FG2/iawGoHVxpYrFCxC5oqjetZpxq/F0AwaLRZ/UGIzzBqri812sqRCP0nG3WA8bzVe6F8JzSWGW3iSgu9sK1fKw1WG8RlGO0f9BvbsMUav7unxmhjTw3Y4yE22bisP2jUfei1iwNnorGNGEe3vi0l3E9SfHw8DQ0NZpchTlZDmRE0NhfsXg5F6yDgNX60NsLGU2us5282QsZqN0KysSwU3tFOAZ0853KM0/jn4mvs/NM7kqC5tpOlWA5vc2U1wtPfDEHfsR4MsSlGuHobobmu8zV2lel/gMlzu3UTEu4i+jU3QOkWKFoPBzZA5W6jp+2INwK6qQpqvz78MSl539wPUHfA6G2mDzGCK+A1gqTfOIjva/TorDZQClBG+BztugqNiB66rr5ZJ+A1etLNDeBMgsR+Ri/Y1wQ+txGYVofR443PNHq8jkQo2w61hcbzeRuNkIvLgLhMiEs3eos+txFmcRlGr7d8h/Fjj4fMkcY6QZ/RJhar0YsP+KB0q/FPzOqAGLuxXFkgeYDRy68vMWr2Nxt1NlYYj+s/2eiV2+Og5mvjd1NWaK43liUPMB57cLNRf//J0HesUafNaaxXtt1oW5vLeP7GcuMyY5jRBj638c/Y22isZ3UYPfaEfsanFVSorfoYv0PhOmO5v9n4h5A5EvqMNtrSU2P87oder1aXAS80lIPfbfzuCf2M16KuxPhnEZ9hXDY3GM/TXP/Na+N3G3V6G8BTBwNO7/a3vdK6k/+Ju9ikSZN02+kHtm/fzogRI0ypB+D+++9n4MCB/PCHPwRg3rx5JCQk8L3vfY8ZM2ZQXV2Nz+fjl7/8JTNmzAA67rlfeeWVFBYW4vF4uOeee7jjjjsAeO+993jwwQcJBAKkp6fzwQcf0NDQwN1338369etRSvHQQw8xa9as467f7PbrEcrzYe9qI2BKt8K+T6Cu6Jv7k/obYRObavyhBbxGgPYdawxpeOogeyJkDDXvdxDiKJRSG7TWk461Xo/tuf9iyVa2FXftx6WR/RJ56PJRHd4/Z84c7r333kPh/sYbb/Dee+/hdDpZtGgRiYmJVFRUcPrpp3PFFVcc9UvM559/ntTUVNxuN6eddhqzZs0iGAwyd+5cVq9eTV5eHlVVVQA88sgjJCUlsXnzZgCqq6u78LeOYu5qqD0AJRvh68/g6zVGr7xFXAbkngN9boXMUUZoJ/QxrVwhwqnHhrsZxo8fT1lZGcXFxZSXl5OSksKAAQPw+Xw8+OCDrF69GovFwoEDBygtLaVv374dPteTTz7JokWLACgsLGTXrl2Ul5dz7rnnHtoXPTU1FYDly5fz2muvHXpsSkpKN/6WEc5TC5vfhC9fhuIvv1kemwL9p8DkO2DYpUbP3B4X+lgtRO/TY8P9aD3s7nT11Vfz5ptvcvDgQebMmQPAggULKC8vZ8OGDdhsNnJzc4+6T/nKlStZvnw5n332GS6Xi/PPPx+Px4PWut3efkfLRYjWsP8T+OJl2Pa2MX6ZORK+9TNIzTN65elDwdLj9uwVwjQ9NtzNMmfOHObOnUtFRQWrVq0CoLa2lszMTGw2GytWrGD//qPPuFlbW0tKSgoul4sdO3awZs0aAM444wzuvPNO9u7de2hYJjU1lalTp/LnP/+ZJ554AjCGZaT3jvGF14YX4fOnoarA+GJq3ByYcBP0myC9ciGOQro6bYwaNYr6+nqys7PJysoC4IYbbmD9+vVMmjSJBQsWMHz48KM+x7Rp0/D7/YwdO5af/exnnH668c14RkYG8+fPZ+bMmYwbN47Zs2cD8NOf/pTq6mpGjx7NuHHjWLFiRff+kj2d1kYP/U8T4d37jLHzK5+B/8mHy58wxs4l2IU4KtlbJopEfPt5m+CTP8L2JVC21dg9beojMOgCCXMhQiJ+bxnRy9QVw2vXQ/FGGHgmXPY4TLgFrPIWFeJEyF+OMJe3ET5+Aj57yrg95xUYPt3cmoSIAhLuwjx7PoTF/2UcVTnqKrjgp5B+itlVCREVJNyFOfLfg9dvhNRB8J1lYTkcW4jeRMJdhN+2xbDwdmO2v5vfNmbQE0J0KdkVUoSP1sbY+hs3G3O53LRIgl2IbiLh3kpNTQ1/+ctfTuix06dPp6amposriiLBALx7Pyx7EEZ8G259B1ypZlclRNSScG/laOEeCASO+tilS5eSnCy90HZ5G43x9bV/hTPugmteMqZaFUJ0Gwn3Vh544AH27NnDqaeeyn333cfKlSu54IILuP766xkzZgxgTOU7ceJERo0axfz58w89Njc3l4qKCvbt28eIESOYO3cuo0aNYurUqbjd7iO2tWTJEqZMmcL48eO56KKLKC0tBaChoYHbbruNMWPGMHbsWBYuXAgYUwVPmDCBcePGceGFF4ahNbqI3wsvz4Sd7xknKLjkVzIHjBBh0HO/UH33AWMC/67Udwxc+miHdz/66KNs2bKFjRs3AsYEYGvXrmXLli2HZnJsbyrftLS0w55n165dvPrqqzz77LNce+21LFy4kBtvvPGwdc4++2zWrFmDUoq//e1v/O53v+Oxxx5rd/rf8vLydqcKjggfPgKFa2DWczDmarOrEaLX6Lnh3kNMnjz5ULBD+1P5tg33vLw8Tj31VAAmTpzIvn37jnjeoqIiZs+eTUlJCV6v99A22pv+d8mSJe1OFdzj7f4APn0SJt4mwS5EmPXccD9KDzuc4uLiDl3vaCrfthyOb058a7Va2x2Wufvuu/nRj37EFVdcwcqVK5k3bx7Q/vS/ETklcH0pLPoeZIyAS35tdjVC9Doy+NlKQkIC9fX1Hd7f0VS+J6K2tpbs7GwAXnzxxUPLW6b/bVFdXc0ZZ5zBqlWr2Lt3L0DPH5YJBmHRHca5JK/5u3FyaSFEWEm4t5KWlsZZZ53F6NGjue+++464v6OpfE/EvHnzuOaaazjnnHNIT08/tLy96X87miq4x/rkCShYCZf+1jhfqRAi7Do15a9SahrwR8AK/E1r/Wib+wcCzwMZQBVwo9a66IgnakWm/O16PaL9CtfC89Ng5Ay4+nmZqleILtbZKX+P2XNXSlmBp4BLgZHAdUqpkW1W+wPwktZ6LPAw8JvjL1lEPHc1vHk7JOUYJ9WQYBfCNJ0ZlpkM7NZaF2itvcBrwIw264wEPghdX9HO/SLaaQ2L74b6Yrj67+BMMrsiIXq1zoR7NlDY6nZRaFlrm4BZoetXAQlKqTROgFlnhop0prfbykeNMyhd9AvImWhuLUKIToV7e5+t2ybJ/wLnKaW+BM4DDgD+I55IqTuUUuuVUuvLy8uPeFKn00llZaX5QRVhtNZUVlbidDrNKWDrv2DVo3DqDXDGnebUIIQ4TGf2cy8C+re6nQMUt15Ba10MzARQSsUDs7TWtW2fSGs9H5gPxheqbe/PycmhqKiI9oJfHJ3T6SQnJyf8G64/CEvugexJ8G0ZZxeip+hMuK8Dhiil8jB65HOA61uvoJRKB6q01kHgxxh7zhw3m8122NGgoofTGt75b/B74KpnIMZudkVCiJBjDstorf3AXcAyYDvwhtZ6q1LqYaXUFaHVzgfylVI7gT7Ar7qpXtGTfPAw5C+FC38O6UPMrkYI0Uqnph/QWi8FlrZZ9vNW198E3uza0kSPtuFF+PhxY96Y039odjVCiDbkCFVx/Nw1sPwhGHg2XPaYjLML0QNJuIvj9/H/GQE/7ddgsZpdjRCiHRLu4viU74TPn4GxsyFrnNnVCCE6IOEuOs/ngTe/AzYXXDTP7GqEEEfRc+dzFz3Psh9D6Wa47nVIzDK7GiHEUUjPXXTO2mdh/fNw5n/BsGlmVyOEOAYJd3FshWvh3fth6KUyHCNEhJBwF0fnbTROl5eYDTPny94xQkQIGXMXR7f8F1BVALcsAWei2dUIITpJeu6iYwWrYO1fYcr3Ie9cs6sRQhwHCXfRPk8dvH0npJ0CFz5kdjVCiOMkwzKifcsehLoD8J33we4yuxohxHGSnrs40s5l8OXLcNa90P80s6sRQpwACXdxuOKNsHAuZI6C8x8wuxohxAmScBffqNwDL19l7BVz/esQ4zC7IiHECZJwFwZvE7x+E6Dh5rchuf8xHyKE6LnkC1UBwQAsvgvKtsGNb0LaYLMrEkKcJOm593bBACy+G7YshIseglMuMrsiIUQXkHDv7ZbPg40L4Pwfw9n/bXY1QoguIuHem331Bnz6JEy6XfaMESLKSLj3Vvnvwr9+aJwH9dLfml2NEKKLSbj3RjvfN/aMyRoLcxaA1WZ2RUKILibh3tvs/xTeuAn6jIQb34LYZLMrEkJ0Awn33qTkK3hlNiT1l2AXIspJuPcW5TvhHzPBkQg3/wvi0s2uSAjRjSTce4O9H8FzFxvXb/4XJOWYW48QottJuEe7PR8aPfaEvvDd5ZA+xOyKhBBhINMPRLP9n8Kr10P6MLhlMbhSza5ICBEm0nOPVge+gAXXGkMwNy2SYBeil5Fwj0b7PjGm7nWlGj32+AyzKxJChJmEe7TZ+Aq8NAPiM41gT+xndkVCCBPImHu00BpW/ApW/x7yzoNrX4TYFLOrEkKYRMI9Wnz8f0awT7gZLntcphQQopeTcI8Gm9+ED34Bo6+Gy58EpcyuSAhhMhlzj3QbXoC35sKAM2DGUxLsQghAwj1yaQ2r/wBL7oHBFxpzxdicZlclhOghZFgmEgWDsOxB+PxpGDvb6LHLGLsQopVO9dyVUtOUUvlKqd1KqSNO2aOUGqCUWqGU+lIp9ZVSanrXlyoA8Hlg0feMYD/9h3DlMxLsQogjHLPnrpSyAk8BFwNFwDql1GKt9bZWq/0UeENr/bRSaiSwFMjthnp7t9JtsPB2KNsGF/4czv6RjLELIdrVmWGZycBurXUBgFLqNWAG0DrcNZAYup4EFHdlkQIoXGdMABbjhOv/CUOnml2REKIH60y4ZwOFrW4XAVParDMPeF8pdTcQB1zUJdUJw9efwz9mGXOw3/qOTNkrhDimzoy5t/e5X7e5fR3wgtY6B5gOvKyUOuK5lVJ3KKXWK6XWl5eXH3+1vdH+T40ee3wm3LZUgl0I0SmdCfcioH+r2zkcOexyO/AGgNb6M8AJHHGqH631fK31JK31pIwMmczqmPZ+ZPTYE/sZwS7zxAghOqkz4b4OGKKUylNK2YE5wOI263wNXAiglBqBEe7SNT8ZO9+HBddA8gC49d/GyTaEEKKTjhnuWms/cBewDNiOsVfMVqXUw0qpK0Kr/Q8wVym1CXgVuFVr3XboRnTWuufg1dmQMRRueccYkhFCiOPQqYOYtNZLMXZvbL3s562ubwPO6trSeqm1z8LS/4Uhl8DVz4Mj3uyKhBARSI5Q7SmCQfj4cfjwERg2Ha55EWLsZlclhIhQEu49QTAA/7wFti+B0bPgyqcl2IUQJ0XCvSdY/Qcj2C/6BZx1jxx1KoQ4aTIrpNn2fAgrfwNj50iwCyG6jIS7mYo3wus3QeYIuOwxCXYhRJeRcDdL8ZfGAUqxqXDjQtkrRgjRpSTczVCwEl74NthccPO/5MhTIUSXk3APt62Lvjny9Pb3IW2w2RUJIaKQhHs4ffEy/PM2yJ4Ymismy+yKhBBRSsI9XD6fD4vvgsHfMs53GptidkVCiCgm4R4On/wR3r0Phl0G170KdpfZFQkhopwcxNSdtIZVvzX2Yx81E2bOl/OdCiHCQsK9uwR8sOxBWDsfTr0BrvgTWKxmVyWE6CUk3LtDUxW8eh0UroHT74SpvwSLjIAJAVDZ0MyagioSnDGcOTiNGKvxt1Hn8ZHgiMEbCLJhfzWjspJIctnw+AI4YiyoThzkp7Xmsz2VxDtjGJuT3O79FQ1e0uPth55Pa403EMQRY6W60UtlYzP9U11oDeX1zeyvbCLOYSUx1sbXVU2gwRFjoaLRy6D0OEZnJx22jXqPD6tF4bLHUFrnobjGTV56HN5AkO0l9Xy4vZSZE3IY1//I+rqShHtXc1fDSzOgPB9mPQdjrja7ItEFyuubiXfEEGtv/9NXncdHvD0Gi8UIjE92V1BW7+HS0VnEWBSF1W4Kq5oYl5NMvDOGdfuqaPYHsSpFVZMXl81KrN3KgRo3iU4b/VNjCQYhPcFO30Qn/qDmxU/38cb6Qk7JjOfcIRmcMzSDzwsq8QWCTBudRVKsjWBQU1Ttpl+y81Botmj2B9hRUs9XRTVsLa4jNz2OmeOzyUhw0OwPUufxkRHvYO3eKp77eC++QJAx2UnMPXcQCU5jODH/YD0Pv7OV0rpm5p6TR2ldMwfrPEzJSyUnJZY4RwwxFsXbG4v5bE8lKXF2/IEgvoBmVL9Eth+s56Nd5bSc7SEjwcGMcf0orG5i2dZSspKceP1BKhu9OGIsZKfEUlDeSFqcndz0OPyBINkpsWTEOyiu9QDG6OfXVY3E2mNo9gXYcbAegBFZiWQnO4lzxOCIsVDT5OOroloO1nmYMCCZ66cMJCnWxuP/2cn2kjrS4x1UNDQf93tjRFYi5w3NwBcI8vneSrYW1xFrszIlL5WPd1fgCxx+agunzcKo7KRuD3dl1jk1Jk2apNevX2/KtrtNTSG8OgcqdsKcV2DIxWZX1CtsK65jTUEl/ZJjmZKXSkqcncZmP6t3lrNhfzW2GAtDMuM5b2gGy7aWsq2kFoDx/VMY0ieeklqjd1XT5EMpyElxMTDNhT+gWZlfxrKtB9lX2YTdamHCwGTOGpxOrN1KTZOPIX3ieX9rKf/eXIIjxsLwrETS4ux8uKMMgARHDM3+IN5AEDD+sFNd9kPB1BkJjhiaA0G8/iCn9k+mrM5zxOPtVgsZCQ7qPT7qPH4yEhxMzkulzu0jOzkWpRRLNhXT0OwHINEZQ53Hf+ixLfX1TXRysM5DZoKD9HgH20rqSHHZmDAghcpGLxsLa0h22eib6DwUovGOmEPP20IpOLV/Mk3NAWwxCoVie0kdGQkOrpmYw/nDMymr8/DWFwdYkV+GI8bKnNP6c6DGjUUpLh3TlzUFlZTUeBiVnURxjZsD1W5irIrCqiYqGrz0S3ZiUQqtoX+qC7fPT0NzgBsmD8DtC/DulhLq3H4avX6afUGSYm0MzoxjaJ8E/rm+iAM1bgD6JTm5akI2ZXXN5KbH0S/ZSWGVG6tFkRpnZ2CaC7c3QK3bx4BUF1aLwuMLkhJnY+3eKt7eWMymwhosFsWEAclMyUujpNbNyvxyLh7Zh3OHZlBY1YQjxkJOqoszBqXhtJ34EK1SaoPWetIx15Nw7yIHvjCC3eeGa180dnkUxy0Y1Ly+vpDCqibuOHcQyS47Hl+AZ1bt4d9flVBa5+HikX0ZkZVAkzfAB9tL2VRUe+jxLruVc4dk8NGuchq9AewxFrTWh/WekmJtBLWm3uNvr4TDxFgUZw9J58zBaVQ0ePl4VwXbSuoAI8C0NgL7ptMHEtTwVVENu8oauPn0gUwZlMbijcUkx9k4JSOezEQn720p4WCth6sm5JCd7CQQhBSXjUZvgKZmP/2SY6l1+zhQ48ZmtVBc46agvAGnzcqUQalcMMw4K9eWA3V8VlDBxIGp2KyKpZsPUlbvwWmzMrxvAh/tqiD/YD0pLhv7Kpvw+AJcNiaLi0b2YWxOEtnJsewpb2TFjjIqGppx2WOIc1j54utqBqbFcfe3TsFlj+Grohqe/WgvOw/W47RZuHhkH24I9XjX7qsiNy2OjAQHOw7WUdngpbHZT5M3wISBKeSlxx3Wls3+ADaL5dCnmxa1bmMYI94RvoEEfyDI3opGimrcTMlLxWU/uW17fAGUAkdM93+vJuEeTjuWwpvfgfgMuP6fkDnc7Ip6tEBQs2RTMfNXF7C7rAGXw8oNUwYwJjuZ5z4uYN2+asAI4elj+vJVUS1bi+s465Q0+iQ4+c/20kPBPC4nicvH9WP6mCxKaj289Nk+VuaXc9GIPlw9MYfTclOwWhRr91bx8e4Kzh2awaSBKWgNW4prKan10C8pln7JTlLj7AQ17K1o5ECNGwWM6pdIWrzjsPqrG70AxDli2FlaT2aig8wEZzib8Li0/HOzx8j3PtFAwj1cCtfBC5dB39Fw3etGwPcyWmtqmnzE2q04QgFS1ejlYJ2Hg7UeSmo9lNZ5qPf4GdY3gYUbili/v5ohmfF8a3gm+yubeG/rQcAYg71v6jDG5CTxpw938dHOCiwWxePXjuPCEX0Ao3ffEu5JLtm1VPQunQ13+UL1ZFTthdeuN6YRuOFNcKWaXVG3a/L6WVNQSf8UFxkJDlbtLOfplXsOjb9aLQqL4ogvkSyhj6xuX4BEZwx/uGYcM8dnH/qIvuVALVWN3sP2nvjLDRPxh8aCW385aLEoCXUhjkHC/USVboOXr4Kgz+ixR2mw769sZOEXB3h3cwkaKKlx0+gNHLbOoPQ47p82HI2mqTmAP6jpk+ggK8lJn0QnWUmxpMfbsShFQUUj6fF2kl2Hn0aw7e5kLdru8SGE6BwJ9xNRVQAvXm4cbXrbu1Exxt7k9WMJ7ff7yDvbWLvX2FXv66omlIIzB6eRFGtjcl4q00b15UCNm6pGL5MGpjApNxWrpXMnGjklU+atFyIcJNyPV2Ml/ONq0AG45T1IH2J2RSfswx2lpMU56JPo5OpnPqW60UtmopO9FY18a3gmTpuFW8/MZeqoPuSkyHw4QkQSCffj4XMbuzvWFsEtiyM22Ju8fh55Zxuvri1EKciId9DkDTB1VF+2FtfyzI0TmTa6r9llCiFOgoR7ZzU3GLs7Fq0z9mMfcLrZFR0XrTVfVzXxn22l/HV1AeX1zfzg/ME0Nvt5e2Mx82+ayJmnpJtdphCii0i4d0bpNiPYK/KNE1mPnGF2RZ32eUElf1m5h42FNdS6fQBMzk3lmRsnMHGg8SXwL64Y1al5O4QQkUPC/Wi0hhW/ho8fB2eScZKNwReYXdURtNasyC/j9XWFpMbZsVoUH++q4GCdB48vSEaCg8vGZjEiK5FzTkknt82RgxLsQkQfCfeOaA3//h9Y/xyMuw4u+XWP291Ra807X5Xw1Ird7DhYb0wA5QvgC2jOOiWdqaP6MjDNxczxOR1OeCWEiE4S7u0pXAvL58H+T+Cse+CiXxgTifQAB2s9fLqngv6pLl7+bD+LNxVzSmY8f7hmHDNO7UeMRREIatk/XIheTsK9NW+TEepr/wrxfeCyx2HSd3pEsPsDQX6yaAtvbCg8NF2qUnDfJcP4wXmDD5uMKcZqfr1CCHNJuLfQGhbdAduXwJQfwIU/A3vcsR8XBgdrPTzy7238+6sSbj0zl6sn5lBc46ZPorPb54QWQkQmCfcWX75sBPvFDxtDMSbz+AL8Y81+3vriwKEpZh+cPpw7zh0MdHy4vhBCgIS7oWI3vHs/5J0HZ9xtdjXUeXx894X1rN1Xxan9k3lw+nDOG5rJsL4JZpcmhIgQEu5+Lyy8HWIccNUzpp7r1OsP8uaGIv66eg/FNW7+dN14Lh/Xz7R6hBCRq3eHu9bw/k+gZCPM/gckmhekZXUefrDgCzbsr2Z0diIv3jZZjhgVQpyw3hvuWsPq38Pa+XDGXTDictNKWb6tlAfe2kxjs58nrxvP5WOz5MAiIcRJ6R3hHgxAQxnUl0DdAeME1lvegtItMHY2XPyIKWVprXn8Pzv504e7Gd43gX98dzLD+yaaUosQIrp0KtyVUtOAPwJW4G9a60fb3P9/QMtx+S4gU2tt/j56DeXw0WOw4QXwuw+/L3MkXPm0Ee5hHGdfv68Kp81KvCOGFz7dxwuf7uPaSTk8cuXosJxcVwjROxwz3JVSVuAp4GKgCFinlFqstd7Wso7W+r9brX83ML4baj0+AZ9xQo2KnUaA50yEhH6Q0BfSBhtzxYTZ5wWVzJ6/5rBlN0wZwCMzRh9xRnghhDgZnem5TwZ2a60LAJRSrwEzgG0drH8d8FDXlHcS1j4L5dth9gIY8W2zq8HrD/KTf20hJyWW+y4ZRp3bxzlDMo6YxEsIIbpCZ8I9GyhsdbsImNLeikqpgUAe8GEH998B3AEwYMCA4yr0uDSUwcrfwOALYfhl3bedTlq7t4qnV+5md1kDf7/1NC4Ynml2SUKIKNeZcG9vvEC3swxgDvCm1jrQ3p1a6/nAfIBJkyZ19BwnR2tYcg/4m2Hao6bOC9PsD/CbpTt44dN9JMXauO+SYRLsQoiw6Ey4FwH9W93OAYo7WHcOcOfJFnXCtIbP/wr5S40pejOGmlZKYVUTd73yBZuKarn97Dzuu2QYTpt8YSqECI/OhPs6YIgKr4iZAAANHklEQVRSKg84gBHg17ddSSk1DEgBPuvSCjurtghemQOlm2HQ+cbkX2HUcsKMj3ZVsLmolq3FdcRYFX+9aSKXjJLzkQohwuuY4a619iul7gKWYewK+bzWeqtS6mFgvdZ6cWjV64DXtNbdM9xyLBtegLKtMOMpGHNt2KcR+OMHu3hi+S6cNgtjs5O5emIOc88ZxIA0V1jrEEII6OR+7lrrpcDSNst+3ub2vK4r6wTkvwsDzoDxN4Z1s8GgZsHn+3li+S5mTcjhNzPHYI+RE2UIIcwVHUeo1nxtHG0a5iNNP91TwbzFW9lZ2sCZg9Mk2IUQPUZ0hHv+e8blsOlh2dzusnr+smIPb315gIFpLp68bjyXjcnCKgciCSF6iOgI953vQtopkH5Kt26msKqJx97P5+1NxThiLHzvvEHce+FQOfm0EKLHiY5wL883TrTRDYqqm9hV1sCq/HIWfL4fq0Xx/fMG892z80iLd3TLNoUQ4mRFR7j7mrr8fKef7K7gd8vy2VRYA4BFwezT+nPPhUPpm+Ts0m0JIURXi5Jwd4P95Hc5rHX7WLypmN2l9by0Zj8DU138+NLhTMpNYWBaHOnSUxdCRIjID/dgEPwesJ18uP/Xq1+yamc5ALMm5PDIlaNw2SO/iYQQvU/kJ1fLPO222JN6mpX5ZazaWc7/mzaM7549SHZpFEJEtMgPd2+TcXmCPfe1e6t45fP9rCmoIjfNJcEuhIgKkR/uvpZwP/6e+/aSOm77+1rsMRb6p7r4yfQREuxCiKgQBeF+YsMyn+6p4EevbyLBaePtu86iT6LsASOEiB5REO7HHpbxBYLUe/zUe3wUVrl58bN9/GdbKQNSXTxz40QJdiFE1ImCcO+45769pI4/fbiL/2wrxRf4ZrLKpFgb9140hO+fN1jmWBdCRKUoCvfDD2IKBDVzX1pPvcfPDVMGkpvmIsFpIynWxpmnpMkujkKIqBb5CdfBF6qrd5VTVO3mz9eP59tj+5lQmBBCmCfydw3pYFhmwZqvSY+3M3WknAVJCNH7REG4NxqXrb5QLa5x8+GOUq6d1F92bRRC9EqRn3zt9Nw/2FFGUMOsiTkmFSWEEOaKgnA/clfINXsqyUpyMii9a2eKFEKISBEF4e4GZQWrDQCtNWsKKjljUBpKyZmRhBC9U3SEuz0OQkG+s7SBykYvpw9OM7kwIYQwTxSEe9Nh4+2f7qkA4EwJdyFELxYF+7m70bZY1hZUsnJnOe9vPUj/1FhyUk5+fnchhIhUkR/u3kaKGmD2/DXYrIq0OAffOTvX7KqEEMJUER/u2uem2hvDZWOz+P3VY2VaASGEIArG3APNjTQG7ZyakyzBLoQQIREf7v7mJtzY6Zsk0/YKIUSLiA/3QHMjTTjIknAXQohDIj7c8bnx4JCeuxBCtBLx4a78btzYyUyQcBdCiBYRH+7WgAdlc8nsj0II0UpkJ6LW2IIeYhxywJIQQrQW2eHu92BBY4+NN7sSIYToUSI73ENzuTsl3IUQ4jARHe5NjfUAuOISTK5ECCF6logO94rqagDiEhJNrkQIIXqWToW7UmqaUipfKbVbKfVAB+tcq5TappTaqpR6pWvLbF9VTS0AiRLuQghxmGNOxqKUsgJPARcDRcA6pdRirfW2VusMAX4MnKW1rlZKZXZXwa1V1xrhnpQo4S6EEK11puc+GdittS7QWnuB14AZbdaZCzylta4G0FqXdW2Z7fO6GwAZcxdCiLY6E+7ZQGGr20WhZa0NBYYqpT5RSq1RSk3rqgKPJtDcCIDDJXvLCCFEa52ZI7e9s0zrdp5nCHA+kAN8pJQarbWuOeyJlLoDuANgwIABx11sW8HmJgDszriTfi4hhIgmnem5FwH9W93OAYrbWedtrbVPa70XyMcI+8NoredrrSdprSdlZGScaM3f8BrDMsouR6gKIURrnQn3dcAQpVSeUsoOzAEWt1nnX8AFAEqpdIxhmoKuLLQ9aY27acQJCVndvSkhhIgoxwx3rbUfuAtYBmwH3tBab1VKPayUuiK02jKgUim1DVgB3Ke1ruyuoltkN24l3zIELNbu3pQQQkSUTp2XTmu9FFjaZtnPW13XwI9CP+Hhc5Pt2c1njquYELaNCiFEZIjcI1RLvsJKgALnCLMrEUKIHidyw71oHQCFrpEmFyKEED1PRId7qSUTrzMsB8MKIUREicxw1xqK1rFVDcFlly9ThRCircgM9+IvoO4AnzJOwl0IIdoRmeG+eSFYbCwNnIbTJuEuhBBtRV64B4Ow9S0YcjFlXqf03IUQoh2RF+5ffwb1JfhHzsQf1BLuQgjRjsgL96J1YIujceDFAMTaO3UclhBC9CqRl4xn3wsTbqbJ5wCQnrsQQrQj8nruAK5U3N6AcVXCXQghjhCZ4Q40hcI9VvaWEUKII0RsuLt9oXCXnrsQQhwhYsO9SYZlhBCiQxEb7m6vH4BYW+R9JyyEEN0tYsNdeu5CCNGxiA33ljF3CXchhDhS5Ia7V75QFUKIjkRsuMuukEII0bGIDne71UKMNWJ/BSGE6DYRm4xur1+GZIQQogMRG+5N3oB8mSqEEB2I2HB3+wLScxdCiA5EbrhLz10IIToUseHe5A3gkqNThRCiXZEb7jIsI4QQHYrYcHd7/bKPuxBCdCBiw132lhFCiI5F3KD1G+sKefajAkpqPTIsI4QQHYi4cE922RjSJ56hfRKYOSHb7HKEEKJHirhwnzqqL1NH9TW7DCGE6NEidsxdCCFExyTchRAiCkm4CyFEFJJwF0KIKCThLoQQUUjCXQghopCEuxBCRCEJdyGEiEJKa23OhpUqB/af4MPTgYouLKcr9dTapK7jI3Udv55aW7TVNVBrnXGslUwL95OhlFqvtZ5kdh3t6am1SV3HR+o6fj21tt5alwzLCCFEFJJwF0KIKBSp4T7f7AKOoqfWJnUdH6nr+PXU2nplXRE55i6EEOLoIrXnLoQQ4igiLtyVUtOUUvlKqd1KqQdMrKO/UmqFUmq7UmqrUuqe0PJ5SqkDSqmNoZ/pJtS2Tym1ObT99aFlqUqp/yildoUuU8Jc07BWbbJRKVWnlLrXrPZSSj2vlCpTSm1ptazdNlKGJ0Pvua+UUhPCXNfvlVI7QttepJRKDi3PVUq5W7XdM2Guq8PXTin141B75SulLumuuo5S2+ut6tqnlNoYWh6WNjtKPoTvPaa1jpgfwArsAQYBdmATMNKkWrKACaHrCcBOYCQwD/hfk9tpH5DeZtnvgAdC1x8Afmvy63gQGGhWewHnAhOALcdqI2A68C6ggNOBz8Nc11QgJnT9t63qym29ngnt1e5rF/o72AQ4gLzQ36w1nLW1uf8x4OfhbLOj5EPY3mOR1nOfDOzWWhdorb3Aa8AMMwrRWpdorb8IXa8HtgM9+bx/M4AXQ9dfBK40sZYLgT1a6xM9iO2kaa1XA1VtFnfURjOAl7RhDZCslMoKV11a6/e11v7QzTVATnds+3jrOooZwGta62at9V5gN8bfbthrU0op4Frg1e7afgc1dZQPYXuPRVq4ZwOFrW4X0QMCVSmVC4wHPg8tuiv00er5cA9/hGjgfaXUBqXUHaFlfbTWJWC88YBME+pqMYfD/9jMbq8WHbVRT3rffQejh9ciTyn1pVJqlVLqHBPqae+160ntdQ5QqrXe1WpZWNusTT6E7T0WaeGu2llm6u4+Sql4YCFwr9a6DngaGAycCpRgfCQMt7O01hOAS4E7lVLnmlBDu5RSduAK4J+hRT2hvY6lR7zvlFI/AfzAgtCiEmCA1no88CPgFaVUYhhL6ui16xHtFXIdh3ckwtpm7eRDh6u2s+yk2izSwr0I6N/qdg5QbFItKKVsGC/cAq31WwBa61KtdUBrHQSepRs/jnZEa10cuiwDFoVqKG35mBe6LAt3XSGXAl9orUtDNZreXq101Eamv++UUrcA3wZu0KFB2tCwR2Xo+gaMse2h4arpKK+d6e0FoJSKAWYCr7csC2ebtZcPhPE9Fmnhvg4YopTKC/UA5wCLzSgkNJb3HLBda/14q+Wtx8muAra0fWw31xWnlEpouY7xZdwWjHa6JbTaLcDb4ayrlcN6Uma3VxsdtdFi4ObQHg2nA7UtH63DQSk1DbgfuEJr3dRqeYZSyhq6PggYAhSEsa6OXrvFwByllEMplReqa2246mrlImCH1rqoZUG42qyjfCCc77Hu/ta4q38wvlXeifEf9ycm1nE2xsemr4CNoZ/pwMvA5tDyxUBWmOsahLGnwiZga0sbAWnAB8Cu0GWqCW3mAiqBpFbLTGkvjH8wJYAPo9d0e0dthPGR+anQe24zMCnMde3GGI9teZ89E1p3Vug13gR8AVwe5ro6fO2An4TaKx+4NNyvZWj5C8D326wbljY7Sj6E7T0mR6gKIUQUirRhGSGEEJ0g4S6EEFFIwl0IIaKQhLsQQkQhCXchhIhCEu5CCBGFJNyFECIKSbgLIUQU+v9rYZ5IEYMSfwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(r.history['val_acc'], label = \"val acc\")\n",
    "plt.plot(r.history['acc'], label = \"train acc\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I create this model as a precomputation step. The subsequent prediction calculations will use the same bidirectional_model output\n",
    "#given an ecoder_input. So we can save time.\n",
    "\n",
    "#Input: encoder_input\n",
    "\n",
    "bidirectional_model = Model(inputs = encoder_input, outputs = encoder_rnn)\n",
    "\n",
    "#Input encoder_rnn\n",
    "\n",
    "rnn_output_placeholder = Input([MAX_ENCODER_SEQUENCE_LENGTH, 2*LATENT_ENCODER_DIM])\n",
    "\n",
    "\n",
    "#Will hold the last predicted word\n",
    "decoder_input_pred = Input([1], name=\"decoder_input_pred\")\n",
    "decoder_input_pred_embedded = decoder_embeddings(decoder_input_pred)\n",
    "\n",
    "\n",
    "alphas_pred,context_pred = attention_layer(h_init, rnn_output_placeholder)\n",
    "context_pred_concatenated = concatenate_context_and_input([context_pred, decoder_input_pred_embedded])\n",
    "o_curr, h_curr, c_curr = lstm_decoder(context_pred_concatenated, initial_state = [h_init, c_init])\n",
    "probs_pred = dense_decoder(o_curr)\n",
    "#translator_model = Model(inputs = [rnn_output_placeholder, h_init], outputs = [h_ss, a_ss, context_pred])\n",
    "translator_model = Model(inputs = [rnn_output_placeholder, decoder_input_pred, h_init, c_init], outputs = [probs_pred, h_curr, c_curr, context_pred, alphas_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    \n",
    "    encoder_sequence = encoder_tokenizer.texts_to_sequences([sentence])\n",
    "    encoder_sequence_padded = pad_sequences(encoder_sequence, maxlen = MAX_ENCODER_SEQUENCE_LENGTH, padding = \"post\")\n",
    "    \n",
    "    decoder_input_pred_ = np.array([[decoder_word2idx['<sos>']]])\n",
    "    h_init_ = np.zeros([1, LATENT_DECODER_DIM])\n",
    "    c_init_ = np.zeros([1, LATENT_DECODER_DIM])\n",
    "    output_sentence = []\n",
    "    attention_matrix = np.zeros((MAX_DECODER_SEQUENCE_LENGTH, MAX_ENCODER_SEQUENCE_LENGTH))\n",
    "    rnn_output_ = bidirectional_model.predict(encoder_sequence_padded);\n",
    "    #print(np.sum(rnn_output_), rnn_output_.shape)\n",
    "    #rnn_output_ = np.random.rand(1, 5, 300)\n",
    "    #print(rnn_output_.shape, np.sum(rnn_output_))\n",
    "    for t in range(MAX_DECODER_SEQUENCE_LENGTH):  \n",
    "        #print(decoder_input_pred_)\n",
    "        probs_pred_, h_init_, c_init_, context_prd_, alphas_pred_ = translator_model.predict([rnn_output_, decoder_input_pred_, h_init_, c_init_])\n",
    "        attention_matrix[t] = alphas_pred_[0,:,0]\n",
    "        #print(\"alphas:\", alphas_pred_[0,:,0].shape)\n",
    "        #print(\"output\", np.sum(probs_pred_), np.sum(h_init_), np.sum(c_init_), np.sum(context_prd_))\n",
    "        word_idx = np.argmax(probs_pred_[0][1:]) + 1;\n",
    "        word = decoder_idx2word[word_idx];\n",
    "        \n",
    "        decoder_input_pred_[0, 0] = word_idx;\n",
    "        output_sentence.append(word)\n",
    "        \n",
    "        if(word == '<eos>'):\n",
    "            break;\n",
    "\n",
    "\n",
    "\n",
    "    return attention_matrix, \" \".join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index = 0\n",
    "#english_sentence = english_sentences[index]\n",
    "#attention_matrix, spanish_sentence = translate(english_sentence)\n",
    "#spanish_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_matrix(attention_matrix, english_sentence, spanish_sentence):\n",
    "    encoder_sequence = encoder_tokenizer.texts_to_sequences([english_sentence])\n",
    "    labels = [''] + [encoder_idx2word[idx] for idx in encoder_sequence[0]] + ['']*(MAX_ENCODER_SEQUENCE_LENGTH - len(encoder_sequence))\n",
    "    decoded_sequence = spanish_sentence.split()\n",
    "    y_labels = [''] + decoded_sequence + ['']*(MAX_DECODER_SEQUENCE_LENGTH - len(decoded_sequence))\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    ax = plt.gca()\n",
    "    plt.imshow(attention_matrix, cmap = 'gray')\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_yticklabels(y_labels)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l sali a comer <eos>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 1000\n",
    "english_sentence = english_sentences[index]\n",
    "encoder_sequence = encoder_tokenizer.texts_to_sequences([english_sentence])\n",
    "attention_matrix, spanish_sentence = translate(english_sentence)\n",
    "print(spanish_sentence)\n",
    "plot_attention_matrix(attention_matrix, english_sentence, spanish_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continue? y/ny\n",
      "Original: I'm a liar.\n",
      "Translation: soy un mentiroso <eos>\n",
      "Continue? y/ny\n",
      "Original: I'm hungry.\n",
      "Translation: me pica el bagre <eos>\n",
      "Continue? y/ny\n",
      "Original: That's not true.\n",
      "Translation: no es verdad <eos>\n",
      "Continue? y/ny\n",
      "Original: That's a lot!\n",
      "Translation: es un montn! <eos>\n",
      "Continue? y/ny\n",
      "Original: He is old.\n",
      "Translation: l es anciano <eos>\n",
      "Continue? y/nn\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    res = input(\"Continue? y/n\");\n",
    "    if(res == 'y'):\n",
    "        rand_index = np.random.randint(MAX_SENTENCES);\n",
    "        english_sentence = english_sentences[rand_index];\n",
    "        encoder_sequence = encoder_tokenizer.texts_to_sequences([english_sentence]);\n",
    "        attention_matrix, spanish_sentence = translate(english_sentence);\n",
    "        print(\"Original:\", english_sentence)\n",
    "        print(\"Translation:\", spanish_sentence)\n",
    "    else:\n",
    "        break;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
